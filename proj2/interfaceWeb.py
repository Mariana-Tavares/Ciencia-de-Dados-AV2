import streamlit as st
import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# --- CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(
    page_title="Assistente M√©dico Especializado",
    page_icon="ü©∫",
    layout="centered"
)

# --- T√çTULO E DESCRI√á√ÉO ---
st.title("ü©∫ Assistente M√©dico Especializado")
st.write(
    "Interface para interagir com o modelo de linguagem `mistral-7b-medico-finetuned`."
)
st.write(
    "Fa√ßa sua pergunta m√©dica diretamente no campo abaixo."
)
st.write("---")

# --- 1. CARREGAMENTO DO MODELO (CACHEADO) ---
# Usamos o cache do Streamlit para carregar o modelo apenas uma vez.

@st.cache_resource
def carregar_modelo():
    """Carrega o modelo fine-tuned e o tokenizador."""
    
    # Caminhos para os modelos
    modeloBase = "mistralai/Mistral-7B-Instruct-v0.2"
    modeloAdaptado = "mistral-7b-medico-finetuned"

    # Carrega o tokenizador
    tokenizador = AutoTokenizer.from_pretrained(modeloBase, trust_remote_code=True)
    tokenizador.pad_token = tokenizador.eos_token
    tokenizador.padding_side = "right"

    # Carrega o modelo base em 4-bit
    modelo_base = AutoModelForCausalLM.from_pretrained(
        modeloBase,
        load_in_4bit=True,
        torch_dtype=torch.bfloat16,
        device_map="auto"
    )

    # Combina o modelo base com os pesos do fine-tuning
    modelo_final = PeftModel.from_pretrained(modelo_base, modeloAdaptado)
    # Junta as camadas para otimizar a performance
    modelo_final = modelo_final.merge_and_unload()
    
    return modelo_final, tokenizador

# Exibe uma mensagem enquanto o modelo carrega
with st.spinner("Carregando o modelo especializado... Isso pode levar alguns minutos na primeira vez."):
    modelo, tokenizador = carregar_modelo()

st.success("‚úÖ Modelo carregado com sucesso!")
st.write("---")

# --- 2. INTERFACE DO USU√ÅRIO ---

# Usando st.form para agrupar o campo e o bot√£o
with st.form("chat_form"):
    st.subheader("Fa√ßa sua pergunta ao assistente")
    
    # Campo para a instru√ß√£o (pergunta principal)
    instrucao_usuario = st.text_input(
        "Pergunta:", 
        placeholder="Ex: Quais s√£o os sintomas de apendicite?"
    )
    
    # Bot√£o de envio dentro do formul√°rio
    submitted = st.form_submit_button("Gerar Resposta")

# --- 3. L√ìGICA DE GERA√á√ÉO DE RESPOSTA ---

if submitted:
    if not instrucao_usuario:
        st.warning("Por favor, digite uma pergunta.")
    else:
        with st.spinner("O modelo est√° pensando..."):
            # Formata o prompt, agora sem o campo de contexto do usu√°rio
            prompt = f"""### Instru√ß√£o:
{instrucao_usuario}

### Contexto:


### Resposta:"""

            # Codifica o prompt e envia para a GPU
            inputs = tokenizador(prompt, return_tensors="pt", return_attention_mask=True).to("cuda")

            # Gera a resposta
            outputs = modelo.generate(
                **inputs, 
                max_new_tokens=512,
                pad_token_id=tokenizador.eos_token_id
            )
            
            # Decodifica a resposta, pulando o prompt inicial
            texto_gerado = tokenizador.decode(outputs[0], skip_special_tokens=True)
            inicio_resposta = texto_gerado.find("### Resposta:") + len("### Resposta:")
            resposta_limpa = texto_gerado[inicio_resposta:].strip()

            # Exibe a resposta
            st.subheader("Resposta do Modelo:")
            st.markdown(resposta_limpa)
